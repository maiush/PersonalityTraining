{
  "config_general": {
    "lighteval_sha": "7ed2636ed53142c0cf5e4aa752970ae62bcf1c24",
    "num_fewshot_seeds": 1,
    "max_samples": null,
    "job_id": "0",
    "start_time": 7227196.799108306,
    "end_time": 7228033.340289969,
    "total_evaluation_time_secondes": "836.5411816630512",
    "model_config": {
      "generation_parameters": {
        "num_blocks": null,
        "block_size": null,
        "early_stopping": null,
        "repetition_penalty": 1.0,
        "frequency_penalty": 0.0,
        "length_penalty": null,
        "presence_penalty": 0.0,
        "max_new_tokens": 2048,
        "min_new_tokens": 0,
        "seed": 123456,
        "stop_tokens": null,
        "temperature": 0.7,
        "top_k": 64,
        "min_p": 0.0,
        "top_p": 0.95,
        "truncate_prompt": null,
        "cache_implementation": null,
        "response_format": null
      },
      "system_prompt": null,
      "cache_dir": "~/.cache/huggingface/lighteval",
      "model_name": "/workspace/models/introspection/gemma-3-4b-it-misalignment",
      "revision": "main",
      "dtype": "bfloat16",
      "tensor_parallel_size": 8,
      "data_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "gpu_memory_utilization": 0.9,
      "max_model_length": 8192,
      "quantization": null,
      "load_format": null,
      "swap_space": 4,
      "seed": 123456,
      "trust_remote_code": false,
      "add_special_tokens": true,
      "multichoice_continuations_start_space": false,
      "pairwise_tokenization": false,
      "max_num_seqs": 2048,
      "max_num_batched_tokens": 65536,
      "subfolder": null,
      "is_async": false,
      "override_chat_template": null
    },
    "model_name": "/workspace/models/introspection/gemma-3-4b-it-misalignment"
  },
  "results": {
    "leaderboard|mmlu:philosophy|5": {
      "acc": 0.617363344051447,
      "acc_stderr": 0.027604689028581986
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "acc": 0.7254150702426565,
      "acc_stderr": 0.015959829933084032
    },
    "leaderboard|mmlu:international_law|5": {
      "acc": 0.71900826446281,
      "acc_stderr": 0.04103203830514512
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "acc": 0.264804469273743,
      "acc_stderr": 0.01475690648326066
    },
    "leaderboard|mmlu:public_relations|5": {
      "acc": 0.6545454545454545,
      "acc_stderr": 0.04554619617541054
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "acc": 0.6075471698113207,
      "acc_stderr": 0.03005258057955785
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "acc": 0.2980132450331126,
      "acc_stderr": 0.037345356767871984
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "acc": 0.5615384615384615,
      "acc_stderr": 0.025158266016868578
    },
    "leaderboard|mmlu:world_religions|5": {
      "acc": 0.7894736842105263,
      "acc_stderr": 0.0312678171466318
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "acc": 0.6935483870967742,
      "acc_stderr": 0.026226485652553883
    },
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "acc": 0.592436974789916,
      "acc_stderr": 0.03191863374478464
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "acc": 0.4630541871921182,
      "acc_stderr": 0.035083705204426656
    },
    "leaderboard|mmlu:human_aging|5": {
      "acc": 0.6143497757847534,
      "acc_stderr": 0.03266842214289201
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "acc": 0.78,
      "acc_stderr": 0.04163331998932263
    },
    "leaderboard|mmlu:machine_learning|5": {
      "acc": 0.4732142857142857,
      "acc_stderr": 0.047389751192741546
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "acc": 0.5310344827586206,
      "acc_stderr": 0.04158632762097828
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "acc": 0.6641221374045801,
      "acc_stderr": 0.041423137719966634
    },
    "leaderboard|mmlu:nutrition|5": {
      "acc": 0.6503267973856209,
      "acc_stderr": 0.0273053080762747
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "acc": 0.4889705882352941,
      "acc_stderr": 0.03036544647727568
    },
    "leaderboard|mmlu:global_facts|5": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316
    },
    "leaderboard|mmlu:sociology|5": {
      "acc": 0.7164179104477612,
      "acc_stderr": 0.031871875379197986
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "acc": 0.4078014184397163,
      "acc_stderr": 0.02931601177634356
    },
    "leaderboard|mmlu:virology|5": {
      "acc": 0.4939759036144578,
      "acc_stderr": 0.03892212195333045
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "acc": 0.6388888888888888,
      "acc_stderr": 0.04643454608906276
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847415
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "acc": 0.42063492063492064,
      "acc_stderr": 0.025424835086924
    },
    "leaderboard|mmlu:college_medicine|5": {
      "acc": 0.5433526011560693,
      "acc_stderr": 0.03798106566014498
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "acc": 0.696969696969697,
      "acc_stderr": 0.03588624800091707
    },
    "leaderboard|mmlu:formal_logic|5": {
      "acc": 0.373015873015873,
      "acc_stderr": 0.04325506042017086
    },
    "leaderboard|mmlu:marketing|5": {
      "acc": 0.8461538461538461,
      "acc_stderr": 0.02363687331748929
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836
    },
    "leaderboard|mmlu:college_biology|5": {
      "acc": 0.6388888888888888,
      "acc_stderr": 0.040166600304512336
    },
    "leaderboard|mmlu:college_physics|5": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.04389869956808778
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.033247089118091176
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "acc": 0.67,
      "acc_stderr": 0.047258156262526094
    },
    "leaderboard|mmlu:prehistory|5": {
      "acc": 0.6450617283950617,
      "acc_stderr": 0.026624152478845853
    },
    "leaderboard|mmlu:professional_law|5": {
      "acc": 0.363754889178618,
      "acc_stderr": 0.012286991879902882
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.028742040903948496
    },
    "leaderboard|mmlu:astronomy|5": {
      "acc": 0.5789473684210527,
      "acc_stderr": 0.040179012759817494
    },
    "leaderboard|mmlu:business_ethics|5": {
      "acc": 0.57,
      "acc_stderr": 0.049756985195624284
    },
    "leaderboard|mmlu:computer_security|5": {
      "acc": 0.62,
      "acc_stderr": 0.048783173121456316
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "acc": 0.7107843137254902,
      "acc_stderr": 0.031822318676475524
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "acc": 0.5953757225433526,
      "acc_stderr": 0.026424816594009852
    },
    "leaderboard|mmlu:management|5": {
      "acc": 0.7864077669902912,
      "acc_stderr": 0.04058042015646035
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "acc": 0.5669934640522876,
      "acc_stderr": 0.020045442473324227
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "acc": 0.6962025316455697,
      "acc_stderr": 0.029936696387138608
    },
    "leaderboard|mmlu:econometrics|5": {
      "acc": 0.38596491228070173,
      "acc_stderr": 0.04579639422070435
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "acc": 0.7486238532110092,
      "acc_stderr": 0.01859920636028741
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "acc": 0.41,
      "acc_stderr": 0.04943110704237102
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "acc": 0.7177914110429447,
      "acc_stderr": 0.03536117886664742
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235
    },
    "leaderboard|mmlu:security_studies|5": {
      "acc": 0.689795918367347,
      "acc_stderr": 0.029613459872484375
    },
    "leaderboard|mmlu:anatomy|5": {
      "acc": 0.5333333333333333,
      "acc_stderr": 0.043097329010363554
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "acc": 0.64,
      "acc_stderr": 0.048241815132442176
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "acc": 0.5446808510638298,
      "acc_stderr": 0.03255525359340356
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "acc": 0.7616580310880829,
      "acc_stderr": 0.03074890536390986
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "acc": 0.7777777777777778,
      "acc_stderr": 0.02962022787479048
    },
    "leaderboard|mmlu:_average|5": {
      "acc": 0.5679815562356761,
      "acc_stderr": 0.03527170396221012
    },
    "all": {
      "acc": 0.5679815562356761,
      "acc_stderr": 0.03527170396221012
    }
  },
  "versions": {},
  "config_tasks": {
    "leaderboard|mmlu:philosophy|5": {
      "name": "mmlu:philosophy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b0fd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "name": "mmlu:miscellaneous",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1ed0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:international_law|5": {
      "name": "mmlu:international_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b09a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "name": "mmlu:moral_scenarios",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b15a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:public_relations|5": {
      "name": "mmlu:public_relations",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b3fa0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "name": "mmlu:clinical_knowledge",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1a50>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "name": "mmlu:high_school_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b0fd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "name": "mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1ed0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:world_religions|5": {
      "name": "mmlu:world_religions",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b09a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "name": "mmlu:high_school_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b15a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "name": "mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b3fa0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "name": "mmlu:high_school_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1a50>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:human_aging|5": {
      "name": "mmlu:human_aging",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b0fd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "name": "mmlu:us_foreign_policy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1ed0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:machine_learning|5": {
      "name": "mmlu:machine_learning",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b09a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "name": "mmlu:electrical_engineering",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b15a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "name": "mmlu:human_sexuality",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b3fa0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:nutrition|5": {
      "name": "mmlu:nutrition",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1a50>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "name": "mmlu:professional_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b0fd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:global_facts|5": {
      "name": "mmlu:global_facts",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1ed0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:sociology|5": {
      "name": "mmlu:sociology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b09a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "name": "mmlu:professional_accounting",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b15a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:virology|5": {
      "name": "mmlu:virology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b3fa0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "name": "mmlu:jurisprudence",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1a50>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "name": "mmlu:abstract_algebra",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b0fd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "name": "mmlu:elementary_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1ed0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_medicine|5": {
      "name": "mmlu:college_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b09a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "name": "mmlu:high_school_european_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b15a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:formal_logic|5": {
      "name": "mmlu:formal_logic",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b3fa0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:marketing|5": {
      "name": "mmlu:marketing",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1a50>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "name": "mmlu:college_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b0fd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_biology|5": {
      "name": "mmlu:college_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1ed0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_physics|5": {
      "name": "mmlu:college_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b09a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "name": "mmlu:high_school_statistics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b15a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "name": "mmlu:medical_genetics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b3fa0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:prehistory|5": {
      "name": "mmlu:prehistory",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1a50>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_law|5": {
      "name": "mmlu:professional_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b0fd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "name": "mmlu:high_school_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1ed0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:astronomy|5": {
      "name": "mmlu:astronomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b09a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:business_ethics|5": {
      "name": "mmlu:business_ethics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b15a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:computer_security|5": {
      "name": "mmlu:computer_security",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b3fa0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "name": "mmlu:high_school_us_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1a50>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "name": "mmlu:moral_disputes",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b0fd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:management|5": {
      "name": "mmlu:management",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1ed0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "name": "mmlu:professional_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b09a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "name": "mmlu:high_school_world_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b15a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:econometrics|5": {
      "name": "mmlu:econometrics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b3fa0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "name": "mmlu:high_school_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1a50>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "name": "mmlu:college_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b0fd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "name": "mmlu:logical_fallacies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1ed0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "name": "mmlu:college_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b09a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:security_studies|5": {
      "name": "mmlu:security_studies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b15a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:anatomy|5": {
      "name": "mmlu:anatomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b3fa0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "name": "mmlu:high_school_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1a50>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "name": "mmlu:conceptual_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b0fd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "name": "mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b1ed0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "name": "mmlu:high_school_geography",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7cdd194b09a0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    }
  },
  "summary_tasks": {
    "leaderboard|mmlu:philosophy|5": {
      "hashes": {
        "hash_examples": "a6d489a8d208fa4b",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "4872b4637e4e6309",
        "hash_cont_tokens": "0f2ee666e44e1e99"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "hashes": {
        "hash_examples": "4c404fdbb4ca57fc",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "a5a10724549a5d23",
        "hash_cont_tokens": "cde2225feab1933f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:international_law|5": {
      "hashes": {
        "hash_examples": "db2fa00d771a062a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "bd350365c1a79bb0",
        "hash_cont_tokens": "08c5b90de7bfb4f6"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "hashes": {
        "hash_examples": "fd8b0431fbdd75ef",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "88f12ac5faba0d83",
        "hash_cont_tokens": "3d8d91204178613e"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:public_relations|5": {
      "hashes": {
        "hash_examples": "b66d52e28e7d14e0",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "27a2e1b92d2321b0",
        "hash_cont_tokens": "79bf7bd7034f5a0b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "hashes": {
        "hash_examples": "ffbb9c7b2be257f9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "4e5ba7a1b3f4cdc9",
        "hash_cont_tokens": "362ac8015e05a95c"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "hashes": {
        "hash_examples": "dc3ce06378548565",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "5877db87c62f1c83",
        "hash_cont_tokens": "0c99ad73af16c786"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "hashes": {
        "hash_examples": "2fb32cf2d80f0b35",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "1af18110d7fddf6b",
        "hash_cont_tokens": "1036b581d4c5d875"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:world_religions|5": {
      "hashes": {
        "hash_examples": "ecdb4a4f94f62930",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "1b9a41d26186d5a9",
        "hash_cont_tokens": "791ad4f84b7ad232"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "hashes": {
        "hash_examples": "c9136373af2180de",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "3593c14e9d31e1d9",
        "hash_cont_tokens": "2af4fda2c2edb145"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "hashes": {
        "hash_examples": "2118f21f71d87d84",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "220f5c30c70e2f95",
        "hash_cont_tokens": "f00f6101c05d10c4"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "hashes": {
        "hash_examples": "b0661bfa1add6404",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "25c6e28e2acfca47",
        "hash_cont_tokens": "a5f56f61e70bcbcd"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:human_aging|5": {
      "hashes": {
        "hash_examples": "c17333e7c7c10797",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "d9660134e0a7151f",
        "hash_cont_tokens": "76408951e56a9937"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "hashes": {
        "hash_examples": "ed7b78629db6678f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "4318092b1a5661c2",
        "hash_cont_tokens": "80f7f8e9394f59a5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:machine_learning|5": {
      "hashes": {
        "hash_examples": "397997cc6f4d581e",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "685329c2ba4cf79c",
        "hash_cont_tokens": "525d930cd6ef9b2c"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "hashes": {
        "hash_examples": "acbc5def98c19b3f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "df17e86d1af3ef99",
        "hash_cont_tokens": "8c2639efdb73da69"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "hashes": {
        "hash_examples": "4edd1e9045df5e3d",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "49e77fdffa20fa51",
        "hash_cont_tokens": "b0ba87061a160fe9"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:nutrition|5": {
      "hashes": {
        "hash_examples": "71e55e2b829b6528",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "5182a80de3a0c725",
        "hash_cont_tokens": "b0f14d6970153baa"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "hashes": {
        "hash_examples": "c373a28a3050a73a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "2cb7d39c8039e81e",
        "hash_cont_tokens": "bd0476c060beebff"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:global_facts|5": {
      "hashes": {
        "hash_examples": "30b315aa6353ee47",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "8c62109459128d88",
        "hash_cont_tokens": "80f7f8e9394f59a5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:sociology|5": {
      "hashes": {
        "hash_examples": "f6c9bc9d18c80870",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "8e5a5f73c9587d90",
        "hash_cont_tokens": "925d33551cfdaefe"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "hashes": {
        "hash_examples": "50f57ab32f5f6cea",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "ad1c888643c2d958",
        "hash_cont_tokens": "a761ae5010ab4d11"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:virology|5": {
      "hashes": {
        "hash_examples": "bc52ffdc3f9b994a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "759db50d07b8ccaf",
        "hash_cont_tokens": "fb4386a3a58ff06d"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "hashes": {
        "hash_examples": "e956f86b124076fe",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "3cd4b78c9471c588",
        "hash_cont_tokens": "4d66ceb37edb2280"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "hashes": {
        "hash_examples": "4c76229e00c9c0e9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "36e3f7425e01a2f5",
        "hash_cont_tokens": "80f7f8e9394f59a5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "hashes": {
        "hash_examples": "146e61d07497a9bd",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "e9f37b1a51c53209",
        "hash_cont_tokens": "3317342f5295c6bc"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_medicine|5": {
      "hashes": {
        "hash_examples": "c33e143163049176",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "620217a27806f99f",
        "hash_cont_tokens": "9e1883b0fa507997"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "hashes": {
        "hash_examples": "854da6e5af0fe1a1",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "685305864daa35ec",
        "hash_cont_tokens": "f92797f2123efa10"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:formal_logic|5": {
      "hashes": {
        "hash_examples": "8635216e1909a03f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "f655e4dacdc4e185",
        "hash_cont_tokens": "06af4c69e79a06e7"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:marketing|5": {
      "hashes": {
        "hash_examples": "8ddb20d964a1b065",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "dc05a245b7714938",
        "hash_cont_tokens": "7322ddf6fec69c07"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "hashes": {
        "hash_examples": "ce61a69c46d47aeb",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "2a84ebe53fa3e3a2",
        "hash_cont_tokens": "80f7f8e9394f59a5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_biology|5": {
      "hashes": {
        "hash_examples": "3ee77f176f38eb8e",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "30adc5e9fcebe3b0",
        "hash_cont_tokens": "82f589fa684eb343"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_physics|5": {
      "hashes": {
        "hash_examples": "ebdab1cdb7e555df",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "81265696745d42d9",
        "hash_cont_tokens": "7734b37480c60b68"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "hashes": {
        "hash_examples": "666c8759b98ee4ff",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "7e8414688e51407b",
        "hash_cont_tokens": "5b85a882ce131b94"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "hashes": {
        "hash_examples": "182a71f4763d2cea",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "8663f926559a6485",
        "hash_cont_tokens": "80f7f8e9394f59a5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:prehistory|5": {
      "hashes": {
        "hash_examples": "6cc50f032a19acaa",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "e2794c007e8dad04",
        "hash_cont_tokens": "def96288afccaf8d"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_law|5": {
      "hashes": {
        "hash_examples": "a8fdc85c64f4b215",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "5c6a249d2df92d64",
        "hash_cont_tokens": "1fd44e276dc979cd"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "hashes": {
        "hash_examples": "fd6646fdb5d58a1f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "654a69df9c70ddd4",
        "hash_cont_tokens": "f4cdeaabe22d2099"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:astronomy|5": {
      "hashes": {
        "hash_examples": "1302effa3a76ce4c",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "c39e7ff8afb80215",
        "hash_cont_tokens": "7ca40730646f1a3b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:business_ethics|5": {
      "hashes": {
        "hash_examples": "03cb8bce5336419a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "dbf9aaae29f95766",
        "hash_cont_tokens": "80f7f8e9394f59a5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:computer_security|5": {
      "hashes": {
        "hash_examples": "a24fd7d08a560921",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "9424545e5a675541",
        "hash_cont_tokens": "80f7f8e9394f59a5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "hashes": {
        "hash_examples": "95fef1c4b7d3f81e",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "e997ee8aab73b7d7",
        "hash_cont_tokens": "8b72ffbec029e692"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "hashes": {
        "hash_examples": "60cbd2baa3fea5c9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "d05ff35f75476ece",
        "hash_cont_tokens": "9012c235e37fc48a"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:management|5": {
      "hashes": {
        "hash_examples": "2bcbe6f6ca63d740",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "f8565fd6e6f38f23",
        "hash_cont_tokens": "d942c9718c643768"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "hashes": {
        "hash_examples": "bf5254fe818356af",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "751f478a9ddb58cb",
        "hash_cont_tokens": "5b7a2826e907d6cc"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "hashes": {
        "hash_examples": "7e5085b6184b0322",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "1cb0f01bd4a8b89f",
        "hash_cont_tokens": "82b8a97bbe032989"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:econometrics|5": {
      "hashes": {
        "hash_examples": "ddde36788a04a46f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "c16be6bb9b1a9006",
        "hash_cont_tokens": "d3e8ef7b05534084"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "hashes": {
        "hash_examples": "c8d1d98a40e11f2f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "66c26336936a8274",
        "hash_cont_tokens": "63b2b26a77c2769c"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "hashes": {
        "hash_examples": "32805b52d7d5daab",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "b977ad8df3c78296",
        "hash_cont_tokens": "80f7f8e9394f59a5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "hashes": {
        "hash_examples": "956e0e6365ab79f1",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "6e8e6278f57809c9",
        "hash_cont_tokens": "75b4a5e9a89852a4"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "hashes": {
        "hash_examples": "55da1a0a0bd33722",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "db947caa301533f0",
        "hash_cont_tokens": "80f7f8e9394f59a5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:security_studies|5": {
      "hashes": {
        "hash_examples": "514c14feaf000ad9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "39c93ae8a898f793",
        "hash_cont_tokens": "336e6076a13b74a2"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:anatomy|5": {
      "hashes": {
        "hash_examples": "6a1f8104dccbd33b",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "4ea1f33ce4988b3c",
        "hash_cont_tokens": "581ef36adea2669d"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "hashes": {
        "hash_examples": "80fc1d623a3d665f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "30a5e19d6255a869",
        "hash_cont_tokens": "80f7f8e9394f59a5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "hashes": {
        "hash_examples": "8300977a79386993",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "82ce061d354e3619",
        "hash_cont_tokens": "f1d87ede94c2e597"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "hashes": {
        "hash_examples": "1f675dcdebc9758f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "fc6ece2705eab572",
        "hash_cont_tokens": "dfaf3f7b025d233d"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "hashes": {
        "hash_examples": "7dc963c7acd19ad8",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "d7ba3800b66ba82c",
        "hash_cont_tokens": "1f7d6002726e5d5e"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "341a076d0beb7048",
      "hash_full_prompts": "7a9f794caabb3bec",
      "hash_input_tokens": "705ac198e00c1df3",
      "hash_cont_tokens": "863507b458a84996"
    },
    "truncated": 0,
    "non_truncated": 0,
    "padded": 0,
    "non_padded": 0
  }
}